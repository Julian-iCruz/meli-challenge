{
    "Home_description": "\nTeniendo como foco que el equipo de MP SEC de Mercado Libre se encarga de segurizar \ud83d\udd10 las aplicaciones tecnol\u00f3gicas, se crea una plataforma que tiene como fin realizar un an\u00e1lisis exploratorio de los datos de manera interactiva, haciendo que los tiempos de entendimiento de los mismos se reduzcan, ya que no es necesario realizar los t\u00edpicos an\u00e1lisis con Jupyter Notebooks, sino enfocarse en entender los datos \ud83d\udcca de la mejor manera posible para tomar decisiones sobre el preprocesamiento que se le aplicara a estos con el fin de crear Modelos s\u00f3lidos para la detecci\u00f3n de anomal\u00edas que respondan a las necesidades del negocio.\n",
    "Home_find": "\nLa aplicaci\u00f3n se encuentra segmentada con 4 p\u00e1ginas principales mostradas en los siguientes tabs.\n",
    "Home_analysis": "\nLa secci\u00f3n tiene como objetivo el entendimiento de los datos, esto se puede realizar de diferentes maneras, en la pesta\u00f1a:blue[\ud83d\udcca An\u00e1lisis de variables] se puede realizar 4 tipos de exploraci\u00f3n:\n1. Variables cualitativas o categ\u00f3ricas\n2. Variables cuantitativas o Num\u00e9ricas\n3. An\u00e1lisis de variables num\u00e9ricas y categ\u00f3ricas o combinado\n4. Comportamiento de usuario.\n\nDependiente de los datos del set de datos se pueden sacar diferentes conclusiones con cada uno de estos tipos de exploratorio.\nPara conocer un poco m\u00e1s de estos tipos de an\u00e1lisis, rem\u00edtase a la ***secci\u00f3n te\u00f3rica*** de cada uno de ellos en su tab correspondiente.\n",
    "Home_analysis_categorical": "\nEl an\u00e1lisis de variables categ\u00f3ricas es fundamental para entender la estructura y caracter\u00edsticas de los datos categ\u00f3ricos, y para tomar decisiones basadas en los patrones y relaciones encontrados. Adem\u00e1s, este an\u00e1lisis puede ser utilizado para identificar grupos o subconjuntos de datos que comparten caracter\u00edsticas similares, lo que puede ayudar a realizar segmentaciones de mercado o identificar patrones de comportamiento de los consumidores.\n",
    "Definition_categorical": "El tablero de an\u00e1lisis de variables categ\u00f3ricas se pueden encontrar en la pesta\u00f1a :blue[\ud83d\udcca An\u00e1lisis de variables] en el tab :blue[Variables Categ\u00f3ricas]\n\nA mano izquierda se encuentran filtros generales de tiempo, variables categ\u00f3ricas o num\u00e9ricas que son aplicables a todas las graficas y tablas.\nEn la parte superior se encuentra un peque\u00f1o texto introductorio a la secci\u00f3n, seguido de los 3 tipos de exploratorios de datos que se pueden realizar :blue[***An\u00e1lisis univariado***], :blue[***An\u00e1lisis bivariado***], :blue[***An\u00e1lisis multivariado***] ordenados en diferentes tabs.",
    "Nominal_definition": "\nEl orden de las categor\u00edas no afecta la interpretaci\u00f3n que se les da, esto quiere decir que est\u00e1n en igualdad de condiciones y el hecho de pertenecer a una u otra categor\u00eda no marca una tendencia de importancia entre ellas.\n",
    "Definition_ordinals": " \nLas variables categ\u00f3ricas ordinales, las cuales aparte de que denotan caracter\u00edsticas, tienen un orden que afecta su importancia a la hora de interpretar conceptos.\n",
    "Definition_metrics": "\nAlgunas medidas que se pueden utilizar para resumir y analizar datos categ\u00f3ricos:\n\n* :blue[Frecuencia:] Esta es la cantidad de veces que aparece una categor\u00eda espec\u00edfica en un conjunto de datos. La frecuencia relativa se refiere a la proporci\u00f3n de casos que caen en cada categor\u00eda y se puede expresar como un porcentaje.\n\n* :blue[Porcentaje:] Es la frecuencia de una categor\u00eda espec\u00edfica dividida por el n\u00famero total de casos, expresado como un porcentaje.\n\n* :blue[Moda:] Es la categor\u00eda m\u00e1s com\u00fan en un conjunto de datos categ\u00f3ricos. La moda puede ser \u00fatil para identificar la categor\u00eda m\u00e1s popular o dominante en un conjunto de datos.\n\n* :blue[Proporci\u00f3n:] La proporci\u00f3n se refiere a la relaci\u00f3n entre la cantidad de casos en una categor\u00eda espec\u00edfica y la cantidad total de casos. Por ejemplo, si hay 80 mujeres y 120 hombres en un conjunto de datos, la proporci\u00f3n de mujeres es 0.4 o 40%.\n\n* :blue[Coeficiente de contingencia:] Este coeficiente se utiliza para medir la asociaci\u00f3n entre dos variables categ\u00f3ricas. El coeficiente de contingencia puede ser utilizado para evaluar la fuerza de la relaci\u00f3n entre dos variables categ\u00f3ricas, y var\u00eda entre 0 y 1, donde 0 indica una relaci\u00f3n nula y 1 indica una relaci\u00f3n perfecta.\n\n* :blue[Entrop\u00eda:] La entrop\u00eda se utiliza para medir la incertidumbre de una variable categ\u00f3rica. Una variable categ\u00f3rica con una entrop\u00eda alta significa que es dif\u00edcil predecir la categor\u00eda de un caso, mientras que una entrop\u00eda baja significa que es f\u00e1cil predecir la categor\u00eda.\n\n* :blue[\u00cdndice de Gini:] El \u00edndice de Gini es una medida de desigualdad que se utiliza com\u00fanmente en econom\u00eda, pero tambi\u00e9n puede ser aplicado a datos categ\u00f3ricos. El \u00edndice de Gini mide la desigualdad en la distribuci\u00f3n de las categor\u00edas de una variable categ\u00f3rica.\n\n* :blue[An\u00e1lisis de correspondencia:] El an\u00e1lisis de correspondencia es una t\u00e9cnica estad\u00edstica utilizada para explorar las relaciones entre dos o m\u00e1s variables categ\u00f3ricas. El an\u00e1lisis de correspondencia puede ser utilizado para examinar patrones y asociaciones entre variables categ\u00f3ricas y para visualizar estas relaciones en un gr\u00e1fico.\n\n* :blue[An\u00e1lisis de conglomerados:] El an\u00e1lisis de conglomerados es una t\u00e9cnica estad\u00edstica que se utiliza para agrupar casos en funci\u00f3n de sus similitudes y diferencias en varias variables categ\u00f3ricas. El an\u00e1lisis de conglomerados puede ser utilizado para explorar patrones en los datos categ\u00f3ricos y para identificar grupos o cl\u00fasteres de casos similares.\n",
    "analysis_variables": "\nEl objetivo principal del an\u00e1lisis exploratorio de datos es obtener una comprensi\u00f3n inicial de los datos antes de realizar an\u00e1lisis m\u00e1s avanzados o construir modelos. Esto implica examinar y resumir los datos de manera visual y num\u00e9rica, identificar patrones y tendencias, detectar valores at\u00edpicos o errores, y evaluar la distribuci\u00f3n y la relaci\u00f3n entre las variables.\n\nEl an\u00e1lisis exploratorio de datos puede ayudar a los investigadores y analistas a descubrir informaci\u00f3n valiosa en los datos, identificar preguntas de investigaci\u00f3n interesantes, identificar posibles limitaciones de los datos y planificar an\u00e1lisis estad\u00edsticos m\u00e1s avanzados. Adem\u00e1s, el an\u00e1lisis exploratorio de datos puede ayudar a los usuarios a comunicar de manera efectiva los resultados del an\u00e1lisis y presentar hallazgos a audiencias no t\u00e9cnicas.\n",
    "categorical_variables": "\n3 tipos de an\u00e1lisis con los cuales podr\u00e1 explorar las variables categ\u00f3ricas de su dataset, cada an\u00e1lisis se encuentra formado por:\n* Filtros para la selecci\u00f3n de la variable \n* Multiselect para seleccionar las subcategor\u00edas de la variable anteriormente elegida.\n* Filtros num\u00e9ricos dependientes de la cantidad de veces de aparici\u00f3n de una subcategor\u00eda\n* Opci\u00f3n de ver los gr\u00e1ficos en cantidad o porcentaje\n* Opci\u00f3n de visualizaci\u00f3n de gr\u00e1ficas\n* Gr\u00e1fica de conteo o porcentaje de variables categ\u00f3ricas\n* Tabla de conteo y porcentaje dependiente de las variables seleccionadas.\n",
    "univariate_analysis": "\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla volutpat vestibulum bibendum. Nulla sem felis, dignissim quis est dictum, dictum consequat ex. Duis scelerisque erat et tortor efficitur, vel molestie odio tempor. Integer ultrices orci quis est suscipit aliquam. Sed nisl ipsum, lobortis vel pellentesque eget, porttitor eleifend lectus. Aenean pharetra eros a fermentum elementum. Sed auctor vestibulum euismod. Integer sed semper ipsum. In vehicula id turpis et scelerisque. Ut tempor aliquet massa vel congue. Nullam iaculis elit magna, at mollis ante lobortis elementum. Nam porta purus dolor.\n",
    "descriptive": {
        "categorical_descriptive": "\n\n### Categoricas\nLa siguiente tabla muestra algunas m\u00e9tricas importantes de las variables cualitativas o categ\u00f3ricas del dataset.\n\n* ***count:*** Cantidad de electos que tiene la categor\u00eda.\n* ***unique:*** Cuantas subcategor\u00edas \u00fanicas tiene la variable.\n* ***top:*** Subcategor\u00eda que m\u00e1s se repite.\n* ***freq:*** Frecuencia de aparici\u00f3n de la subcategor\u00eda que m\u00e1s se repite.\n* ***type:*** Tipo de la variable.\n* ***Nulls:*** Cantidad de nulos que tiene la variable.\n",
        "numerical_descriptive": "\n\n### Numericas\n"
    },
    "informe": {
        "intro": {
            "join": "\nEl primer paso importante es unificar los datasets que encontramos en [Records of Operations on the CoRE Systems](https://data.mendeley.com/datasets/9k3pkspfxm/1), para ello se toman [Data of First_Round Experiment.xlsx](https://data.mendeley.com/public-files/datasets/9k3pkspfxm/files/13fe969e-619e-4a03-98db-e63d126a2d0d/file_downloaded) y [Data of Second_Round Experiment.xlsx](https://data.mendeley.com/public-files/datasets/9k3pkspfxm/files/9286bf12-acb6-482e-a239-c3a00130be92/file_downloaded) y se concatenan, ya que tienen las mismas columnas. El c\u00f3digo usado para hacer este procedimiento es mostrado a continuaci\u00f3n:\n",
            "code_join": "\ndef joinData():\n    df_firts_round_raw = pd.read_excel(\"Data of First_Round Experiment.xlsx\", sheet_name = \"Raw Records\")\n    df_second_round_raw = pd.read_excel(\"Data of Second_Round Experiment.xlsx\", sheet_name = \"Raw Records\")\n    df_firts_round = deleteNullRowsAndColumns(df_firts_round_raw)\n    df_second_round_raw = deleteNullRowsAndColumns(df_firts_round_raw)\n    df_behavior = pd.concat([df_firts_round, df_second_round_raw], axis = 0)\n    return df_behavior\n\ndef deleteNullRowsAndColumns(df):\n    df.dropna(axis = 0, how = \"all\", inplace=True)\n    df.dropna(axis = 1, how = \"all\", inplace=True)\n    return df.reset_index(drop = True)\n\njoinData()\n",
            "after_join": "\nA continuaci\u00f3n puede descargar el dataset resultado de la ejecuci\u00f3n del c\u00f3digo anterior, el dataset descargado debe ser subido en la seccion :red[***File upload***] del :green[***Sidebar***] para que se puedan ejecutar futuros procesamientos que se indican en esta secci\u00f3n. De igual manera, podr\u00e1 hacer uso de las secciones :blue[descriptivas] y :blue[an\u00e1lisis de variables] para realizar el exploratorio de variables.\n",
            "description": "\nDescripci\u00f3n de las variables que componen nuestro set de datos.\n\n1.  :blue[Date] : Momento en que el usuario realiza una operaci\u00f3n\n2.  :blue[Login ID] : Identificador de inicio de sesi\u00f3n del usuario proporcionado por el sistema CoRE\n3.  :blue[Action] : Incluye el clic del rat\u00f3n sobre un bot\u00f3n o un enlace, y la selecci\u00f3n en un men\u00fa desplegable\n4.  :blue[Page] : La p\u00e1gina web actual en la que se produce la acci\u00f3n\n5.  :blue[Next Page] : La p\u00e1gina siguiente a la que se dirigir\u00e1 el sistema\n6.  :blue[Content] : Contenido de la p\u00e1gina web datos introducidos por el usuario, respuestas del sistema a la acci\u00f3n del usuario.\n7.  :blue[Goal] : Objetivo declarado por el usuario.\n"
        },
        "descriptivo": {
            "observations": "\nLas primeras observaciones relevantes a tener en cuenta realizadas en base a la seccion :blue[Descriptivo] para el exploratorio y an\u00e1lisis de los datos son:\n\n* La variable :red[***Login ID***] muestra que a pesar de que hay :green[65 usuarios], existe uno que tiene aproximadamente la d\u00e9cima parte de los eventos, esto nos indica que si se desea hacer un modelo general para todos los usuarios, posiblemente la muestra se sesgue hacia el comportamiento de dicho usuario. Sin embargo, si se realiza un modelo para entender el comportamiento por usuario y detectar anomal\u00edas, este usuario tiene un amplio hist\u00f3rico de actividad.\n* La variable :red[***Action***] nos permite identificar :green[30 acciones] posibles en la aplicaci\u00f3n, dato de alta relevancia, ya que un ataque puede presentarse por una acci\u00f3n que no se encuentre contemplada entre estas. De igual manera, aproximadamente un 20% de las acciones est\u00e1n ligadas a la obtencion de informaci\u00f3n de un paper espec\u00edfico. Las acciones pueden ser fundamentales a la hora de entender el comportamiento de un individuo, porque habitualmente las personas tienden a realizar los mismos procesos una y otra vez, es por ello que acciones nuevas realizadas por un individuo podr\u00edan tomarse como un comportamiento an\u00f3malo.\n* La App web est\u00e1 compuesta por :green[14 p\u00e1ginas] dato obtenido por las variables :red[***Page***] y :red[***Next Page***] siendo la p\u00e1gina m\u00e1s concurrente por los usuarios la llamada Page_AllPapers. A primera vista puede que estas variables por s\u00ed solas no arrojen informaci\u00f3n para el entendimiento de la data, pero pueden ser de bastante relevancia al juntarlas con otras variables como Action o Goal.\n* La variable :red[***Content***] la cual contiene valores nulos en casi un 40% de sus registros, esto nos da un primer indicio que no toda acci\u00f3n involucra introducci\u00f3n de datos por parte del usuario o respuestas del sistema. Por otro lado, su subcategor\u00eda que m\u00e1s se repite es ***Pre-Session Questionnaire Answers:[Familiar] C*** esto aparte de mostrar cu\u00e1l es la respuesta m\u00e1s recurrente por los usuarios, tiene la capacidad de segmentar una poblaci\u00f3n asociada a dicha respuesta.\n* La variable :red[***Goal***] es interesante, ya que es un input dado por cada usuario cada que realiza una acci\u00f3n, a pesar de que la subcategor\u00eda m\u00e1s repetida sea test, las dem\u00e1s subcategor\u00edas tiene la capacidad de darnos a entender realmente la combinaci\u00f3n de que acci\u00f3n, desde que p\u00e1gina y hac\u00eda que p\u00e1gina genera que objetivo diferente a test entendiendo de esta manera el comportamiento de los usuarios y teniendo la capacidad de decir a futuro cu\u00e1les usuarios est\u00e1n dando un ***Goal*** herrado con respecto a la acci\u00f3n que est\u00e1n realizando denotando de esta manera la realizaci\u00f3n de una acci\u00f3n an\u00f3mala.\n"
        },
        "categorical": {
            "univariate": "\n### Univariado\n\nUna vez se tienen las m\u00e9tricas principales, se puede pasar a realizar una segunda iteraci\u00f3n que consiste en la exploraci\u00f3n m\u00e1s profunda de cada una de las variables con ayuda de la secci\u00f3n \ud83d\udcca :blue[***An\u00e1lisis de variables***] espec\u00edficamente en el tab :green[***Variables Categ\u00f3ricas***] en el subtab :red[***An\u00e1lisis univariado***].\n\n> **Nota:** \nPara cada una de las variables se realiza un analisis similar al de ***Login ID***, para no hacer muy extenso el informe cuando se realicen las tranformaciones se tocaran puntos especificos relevante que sustenten dichas desiciones en base a este tipo de analisis.\n\n#### Login ID\nCada una de las m\u00e9tricas sacadas a continuaci\u00f3n se dan bas\u00e1ndose en el an\u00e1lisis de las gr\u00e1ficas y tablas que pueden obtenerse de nuestro dashboard.\n\n* Mayor cantidad de eventos: :red[user049] | :green[1530] | :blue[11.48 %]\n* Menor cantidad de eventos: :red[user148] | :green[26]   | :blue[0.19 %]\n* Promedio de eventos por usuario: :red[205]\n* La desviaci\u00f3n est\u00e1ndar: :red[201.49] (Nos indica que tanto se aleja la cantidad de eventos realizados por un usuario de la media)\n* La cantidad de eventos por usuario generados en el :red[***Q1***] est\u00e1 entre :green[***26***] y :green[***122***] eventos . :blue[***16***] usuarios se encuentran en el Q1.\n* La cantidad de eventos por usuario generados en el :red[***Q2***] est\u00e1 entre :green[***122***] y :green[***164***] eventos . :blue[***18***] usuarios se encuentran en el Q2.\n* La cantidad de eventos por usuario generados en el :red[***Q3***] est\u00e1 entre :green[***164***] y :green[***204***] eventos . :blue[***15***] usuarios se encuentran en el Q3.\n* La cantidad de eventos por usuario generados en el :red[***Q4***] est\u00e1 entre :green[***204***] y :green[***1530***] eventos . :blue[***16***] usuarios se encuentran en el Q4.\n\nLa distribuci\u00f3n de los Qs nos muestra que tan dispersos est\u00e1n la cantidad de eventos por usuario, :red[Q4] contiene a los usuarios con m\u00e1s cantidad de eventos que se separan de una gran manera de la media de los dem\u00e1s usuarios.\n\nCon los Qs calculados se puede realizar el gr\u00e1fico de cajas que permite ver de manera grafica la distribuci\u00f3n de los eventos. Un aspecto importante a tener en cuenta es que el diagrama de cajas puede identificar muestras an\u00f3malas con ayuda del :red[Rango intercuart\u00edlico], el cual calcula nuevos m\u00e1ximos y m\u00ednimos dada la siguiente f\u00f3rmula.\n\n* RIQ = Q3 - Q1\n\nPara calcular los nuevos m\u00e1ximos y los nuevos m\u00ednimos se utiliza lo siguiente:\n\n* M\u00ednimo = Q1 - 1.5 x RIQ\n* M\u00e1ximo = Q3 + 1.5 x RIQ\n\nA pesar de esto, la cantidad de eventos que genera un usuario con respecto a otro no puede ser considerada una anomal\u00eda. Sin embargo, esto nos da la idea de que se pueden encontrar relaci\u00f3n entre la cantidad de eventos que genera un \u00a0mismo usuario en lapsos de tiempo predefinidos. \n\nCon este an\u00e1lisis entendemos la distribuci\u00f3n de los datos con respecto a la cantidad de evento o acciones que un usuario genera.\n",
            "bivariate": "\n---\n### Bivariado\n\nEntendiendo el comportamiento de cada una de las variables se procede a empezar a realizar relaciones entre ellas con el fin de identificar posibles patrones entre dos variables con ayuda de la secci\u00f3n \ud83d\udcca :blue[***An\u00e1lisis de variables***] espec\u00edficamente en el tab :green[***Variables Categ\u00f3ricas***] en el subtab :red[***An\u00e1lisis biivariado***].\n\n> **Nota:** \nPara cada una de las posibles combinaciones se realiza un analisis similar al de ***Action*** y  ***Goal***, para no hacer muy extenso el informe cuando se realicen las tranformaciones se tocaran puntos especificos relevante que sustenten dichas desiciones en base a este tipo de analisis.\n\n#### Action vs Goal\nCada una de las m\u00e9tricas sacadas a continuaci\u00f3n se dan bas\u00e1ndose en el an\u00e1lisis de las gr\u00e1ficas y tablas que pueden obtenerse de nuestro dashboard. Primero se seleccionan las dos variables a explorar donde se puedn sacar las siguientes observaciones.\n\n* Mayor cantidad de acciones que llevan a un objetivo dar :red[click(Link_PaperInfos)] habitualmente tiene como objetivo :red[View a Paper Info] | :green[2708] | :blue[20.33%].\n* :red[54] Acciones que llevan a un objetivo | :green[2] | :blue[0.015%].\n* El promedio de Acciones y Objetivo en cada evento: :red[65]\n* La desviaci\u00f3n de Acciones por Objetivo en cada evento: :red[201.53] (Nos indica que tanto se aleja la acci\u00f3n y objetivo de cada evento realizado por un usuario de la media)\n* La cantidad de acciones que llevan a un objetivo generadas en el :red[***Q1***] est\u00e1 entre :green[***2***] y :green[***2***] eventos . :blue[***54***] acciones que llevan a un objetivo se encuentran en Q1.\n* La cantidad de acciones que llevan a un objetivo generadas en el :red[***Q2***] est\u00e1 entre :green[***2***] y :green[***8***] eventos . :blue[***50***] acciones que llevan a un objetivo se encuentran en Q2.\n* La cantidad de acciones que llevan a un objetivo generadas en el :red[***Q3***] est\u00e1 entre :green[***8***] y :green[***35***] eventos . :blue[***47***] acciones que llevan a un objetivo se encuentran en Q3.\n* La cantidad de acciones que llevan a un objetivo generadas en el :red[***Q4***] est\u00e1 entre :green[***35***] y :green[***2710***] eventos . :blue[***52***] acciones que llevan a un objetivo se encuentran en Q4.\n\nLa distribuci\u00f3n de los Qs nos muestra que tan dispersos est\u00e1n la cantidad de eventos por usuario,Q4 contiene la mayor cantidad de acciones que llevan a un objetivo ampliamente disperso.\n\nAlgunos de los datos mas importantes que se pueden sacar al usar los selectores multiples de subcategorias son los siguientes:\n\n* Al filtrar la subcategoria :green[click(Link_PaperInfos)] de :red[Action] y cambiando la grafica a Grupo, se puede ver que dicha accion tiene como :red[Goal] mas frecuente :green[View a Paper Info], pero mas que eso se puede ver que la segunda subcategoria de :red[Goal] es :green[Test], dando a entender que los usuarios estaban testeando esta opcion, sin embargo, aparecen otras opciones con baja frecuencia en :red[Goal] como por ejemplo :green[Upload a Paper], esto nos da a entender dos cosas importantes:\n\n    1. La :red[Action] :green[click(Link_PaperInfos)] no genera el :red[Goal] :green[Upload a Paper]. \n    2. La :red[Action] :green[click(Link_PaperInfos)] si genera el :red[Goal] :green[View a Paper Info]. \n\nDe esta manera se pueden encontrar las relaciones reales entre :blue[Action] y :blue[Goal], con ello se podr\u00eda saber de antemano las acciones y sus posibles objetivos, pudiendo as\u00ed crear un dataset que sea capaz de identificar cuando un usuario est\u00e1 generando un :blue[Goal] an\u00f3malo.\n\n",
            "multivariate": "\n---\n### An\u00e1lisis multivariado\n> **Nota:** \nEl an\u00e1lisis multivariado tiende a ser similar casi igual que el bivariado, la diferencia radica en la cantidad de variables que se deseen relacionar al tiempo con el fin de entender el comportamiento de estas de manera conjunta.\n"
        },
        "processing": {
            "target_1": "\n### Inicio de sesi\u00f3n an\u00f3malo\n* :red[***Objetivo***]: Crear un set de datos donde se identifiquen aquellas horas en las que habitualmente inicia sesi\u00f3n un usuario.\n\nSe realiza este caso de uso, ya que se sugiere en el documento del challenge. Para crear el dataset esperado se van a seguir los siguientes pasos:\n\n1. Filtrar los eventos con ***Action*** - ***click(Btn_Login)***\n2. Extraer hora y d\u00eda de la semana en que el usuario genera un inicio de sesi\u00f3n.\n3. Crear la bandera jornada que dir\u00e1 si el horario es diurno 8 AM - 6 PM o nocturno 6 PM - 8 AM.\n4. Crear la bandera d\u00eda de la semana que nos dir\u00e1 si el d\u00eda es fin de semana o entre semana.\n5. Se sacar\u00e1n las horas habituales de inicio de sesi\u00f3n para cada usuario y se crear\u00e1 una variable que diga si el horario es habitual o no.\n\nCon esto se tienen la posibilidad de crear un modelo supervisado creando una variable de m\u00e1s que diga si la hora en la que inicio sesi\u00f3n el usuario es habitual o no, o tambi\u00e9n meter estas variables a un modelo no supervisado con el fin de que internamente este cree una l\u00ednea base de comportamiento de inicio de sesi\u00f3n de usuario y pueda de esta manera identificar eventos an\u00f3malos.\n\nPartiendo del dataset base y con ayuda del c\u00f3digo mostrado a continuaci\u00f3n que realiza los pasos propuestos en la parte superior:\n",
            "code_1": "\ndef procesingLogin(df):\n    df['Date'] = pd.to_datetime(df['Date'])\n    df = df[df[\"Action\"]==\"click(Btn_Login)\"]\n    df['Day'] = df['Date'].dt.day_name()\n    df['Hour'] = df['Date'].dt.hour\n    df['Flag_day'] = df['Day'].isin(['Saturday','Sunday'])\n    df['Flag_hour'] = (df['Hour'] >=8) & (df['Hour'] <=18)\n    user_list = df['Login ID'].value_counts().keys()\n    df['regular_hour'] = df.apply(compareTime, axis = 1, regular_hour_user = obtainRegularHours(df, user_list))\n    return df.drop(['Action','Date', 'Page', 'Next Page', 'Content','Goal'], axis = 1)\n\ndef obtainRegularHours(df_in, user_list):\n    regular_hours_dict = {}\n    for user in user_list:\n        df_user = df_in[df_in[\"Login ID\"] == user]\n        conteo = df_user[\"Hour\"].value_counts()\n        regular_hours_dict[user] = conteo[conteo > 2].index.to_list()\n    return regular_hours_dict\n\ndef compareTime(row, regular_hour_user):\n    return row[\"Login ID\"] in regular_hour_user.keys() and row[\"Hour\"] in regular_hour_user[row[\"Login ID\"]]\n\ndf = procesingLogin(df)\n\n",
            "posprocessing_1": "\nUna vez se obtiene el dataset necesario para crear el modelo, se deben realizar algunos procesos antes de realizar el entrenamiento, la hora ser\u00e1 tratada como variables categ\u00f3ricas nominales(El orden no afecta), ya que si se tratara la hora como variable num\u00e9rica, esta indicar\u00eda que la hora 23 tiene m\u00e1s relevancia que la hora 1 y esto no es correcto.\n\nA las variables categ\u00f3ricas se les deber\u00eda realizar un proceso de codificaci\u00f3n, esto con el fin de que la digesti\u00f3n de los datos sea m\u00e1s sencilla para los modelos. Dejo enunciado un pipeline que es muy habitual para el tratamiento de este tipo de variables.\n\n1. Codificaci\u00f3n de la variable: Existen varios m\u00e9todos con los que se puede realizar dicha codificaci\u00f3n a continuaci\u00f3n 2 de los m\u00e1s usados y explicaci\u00f3n de cuando usar cada uno:\n    * One Hot Encoder: Tiene la capacidad de asignar cada una de las subcategor\u00edas de una variable a una columna del dataframe, poniendo un \u201c1\u201d en el lugar donde la subcategor\u00eda este presente. De esta manera, una categor\u00eda que tiene n subcategor\u00edas, har\u00e1 que el dataframe crezca en n columnas. Esta codificaci\u00f3n es sumamente funcional cuando la cantidad de subcategor\u00edas no es muy grande, de otro modo har\u00eda crecer de gran manera el tama\u00f1o del dataframe llen\u00e1ndolo en ocasiones de muchos ceros que lo \u00fanico que representaran es espacio en memoria.\n    * Binary Encoder: La codificaci\u00f3n binaria es de suma importancia cuando una variable categ\u00f3rica tiene bastantes subcategor\u00edas, puede ser capaz de representarlas n subcategor\u00edas en un n\u00famero reducci\u00f3n de columnas. La f\u00f3rmula para calcular la cantidad de columnas resultantes dependiente de n cantidad de subcategor\u00edas es:\n\n        * $\\log(n + 1)/\\log(2)$\n\n\n2. Reducci\u00f3n de dimensionalidad: Al tener todas las variables numericas puede existir una alta correlacion entre variables,la alta correlaci\u00f3n implica ruido a la hora de entrenar el modelo, porque son variables que no dan valor de informaci\u00f3n a los c\u00e1lculos internos de los modelos. Por este motivo realizar la reducci\u00f3n a estas variables altamente correlacionadas es sumamente importante, el an\u00e1lisis de componente principal (PCA) tiene la capacidad de representar la  informaci\u00f3n de n variables correlacionadas en una serie de componentes, en las cuales se almacena la mayor informaci\u00f3n de las variables iniciales.\n\nCon esto ya se podr\u00eda realizar los dos tipos de modelos mencionados.\n\n1. Modelo supervisado: Donde el target ser\u00eda ***regular_hour***\n2. Modelo no supervisado: Omitiendo la variable ***regular_hour*** se puede generar un modelo que se ajuste a la l\u00ednea base del comportamiento de los usuarios.\n",
            "target_2": "\n---\n### Entendimiento de acciones por contenido\n\n* :red[***Objetivo***]: Crear un set de datos donde se creen nuevas columnas dependientes de los contenidos introducidos por el usuario o las respuestas dadas por el sistema a la acci\u00f3n del uszuario.\n\nPara crear el dataset esperado se van a seguir los siguientes pasos:\n\n1. Renombrar la variable de :blue[Login ID], ya que se vuelve nula al desagregar la columna :blue[content].\n2. Eliminar con ayuda de expresiones regulares aquellos casos donde puede venir dentro del texto caracter\u00edstico como [10], [e], [Programming Techniques] que tergiversar\u00edan la desagregaci\u00f3n correcta de :blue[Content].\n3. Crear un diccionario donde la llave sea lo que contienen los [ ] y el valor sea lo que viene despu\u00e9s de estos. Por ejemplo :red[[Password]1234] -> :green[{\"Password\":\"1234\"}]\n4. Convertir las llaves a columnas con sus respectivos valores, en caso de que una llave no se encuentre en un evento, esta quedara como nula.\n5. Eliminar Filas y columnas que pueden quedar Nulas en su totalidad.\n\nPartiendo del dataset base y con ayuda del c\u00f3digo mostrado a continuaci\u00f3n que realiza los pasos propuestos en la parte superior:\n",
            "code_2": "\ndef deleteNullRowsAndColumns(df):\n    df.dropna(axis = 0, how = \"all\", inplace=True)\n    df.dropna(axis = 1, how = \"all\", inplace=True)\n    return df.reset_index(drop = True)\n\ndef replaceSquareBrackets(text, word):\n    return text.replace(\"[\" + word + \"]\", \"(\" + word + \")\")\n\ndef convertContentDictionary(content):\n    if type(content) == str:\n        content = re.sub(r'\\[(\\d)', r'(\u0001', content)\n        content = re.sub(r'(\\d)\\]', r'\u0001)', content)\n        content = re.sub(r'\\[(\\w)\\]', r'(\u0001)', content)\n        words = [\"Programming Techniques\", \"Programming Languages\", \"Mathematical Logic and Formal Languages\", \"History of Computing\", \" to \", \"JD96b\", \"\"]\n        for word in words:\n            content = replaceSquareBrackets(content, word)\n        return dict(re.findall('\\[(.*?)\\]([^\\[\\]]+)', content))\n    return content\n\ndef procesingActionContent(df):\n    df.rename(columns={\"Login ID\" : \"User ID\"}, inplace=True)\n    df[\"Content\"] = df['Content'].apply(convertContentDictionary)\n    df = pd.concat([df.drop(['Content',], axis=1), df['Content'].apply(pd.Series)], axis=1)\n    return deleteNullRowsAndColumns(df)\n\nprocesingActionContent(df)\n",
            "posprocessing_2": "\n\n"
        },
        "attacks_vulnerabilities": {
            "intro": "\nPartiendo del concepto de que una vulnerabilidad es una debilidad en el sistema de seguridad que podr\u00eda permitir que un atacante comprometa el sistema o acceda a informaci\u00f3n confidencia y un ataque es cuando un individuo intenta explotar una vulnerabilidad en un sistema de seguridad teniendo diferentes objetivos, como obtener informaci\u00f3n confidencial, causar da\u00f1o al sistema o a los usuarios, crear interrupciones en el servicio, entre otros.\n\nCon el dataset que se obtiene en las seccion :blue[Procesamiento de datos] en el caso de uso :blue[Entendimiento de acciones por contenido] y al cula me referire como :red[data_action_content.csv] se puede entender a la perfecci\u00f3n dos tipos de comportamientos que son importantes desde mi perspectiva a la hora de detectar posibles ataques y brechas de seguridad.\n\n1. El comportamiento de cada usuario al realizar diferentes acciones en su d\u00eda a d\u00eda, garantizando de esta manera crear una l\u00ednea base de acciones repetitivas o muy similares a lo largo del tiempo.\n2. El comportamiento de la poblaci\u00f3n general a trav\u00e9s de tiempo, de esta manera se puede entender el comportamiento base por acci\u00f3n. \n\nEl entendimiento de estos dos comportamientos es de suma importancia, ya que, se complementan uno al otro. Imaginemos un caso donde un usuario entra por primera vez a la app web, bas\u00e1ndonos en el comportamiento de la poblaci\u00f3n que ya ha estado usando la app se puede determinar si este nuevo usuario podr\u00eda estar realizando posibles acciones que se salen de lo com\u00fan y que rompen por completo con la finalidad de la misma. A pesar de no contar con datos del usuario, tener un hist\u00f3rico de datos nos permite conocer los movimientos habituales de las personas. \n\nEste ejemplo es algo gen\u00e9rico, pero podr\u00eda ponerse tan complejo como queramos, un claro ejemplo de esto podr\u00eda ser que a la hora del registro se solicitaran datos que fueran capaces de segmentar al sujeto en cuesti\u00f3n, como por ejemplo edad, g\u00e9nero, pa\u00eds, departamento o estado, algunas preguntas de inter\u00e9s para usar la aplicaci\u00f3n, etc. Esto lo que lograr\u00eda es darnos muchas pautas para lograr clasificarlo, pudiendo hacer ejemplo no solo un modelo, sino m\u00faltiples modelos que dado el perfil del usuario permitan saber si su comportamiento es an\u00f3malo o no con respecto a sus s\u00edmiles.\n\nCon base en lo mencionado se escoge la variable :blue[Action] para lograr entender dichos comportamientos tanto a nivel de usuario como a nivel general de la poblaci\u00f3n. Sumado a esto, aprovechamos que el set de datos nos da los inputs del usuario y los outputs del sistema en la vairable :blue[Content] y con ayuda de la desagregaci\u00f3n podemos tener una visi\u00f3n m\u00e1s amplia de cada evento y sus diferentes variantes. \n\nA continuacion un conjunto de vulnerabilidades y ataques que pueden presentarse en el conjunto set de datos.\n\n---\n### Riesgos de privacidad\n\nEl set de datos contiene informaci\u00f3n sobre las acciones del usuario en un sitio web, incluyendo identificadores de inicio de sesi\u00f3n y posiblemente datos sensibles que los usuarios ingresan en el sistema. Si esta informaci\u00f3n se filtrara o se accediera por partes no autorizadas, podr\u00eda ser utilizada con fines maliciosos, como el robo de identidad o el perfilado.\n\nEste caso especifico se puede ver al filtrar por la :red[Action]: :green[click(Btn_Login)] donde el usuario y la contrase\u00f1a no se encuentran encriptadas.\n",
            "attacks_risk": "\nOtro caso en el cual se puede detectar esta particularidad se obtiene al filtrar :red[Action]: :green[click(Btn_Submit)] y :red[Page]: :green[Page_MyProfile], donde el usuario realiza la actualizaci\u00f3n de su password.\n",
            "xss": "\n---\n### Cross Site Scripting\nSi el conjunto de datos incluye contenido que los usuarios ingresaron en el sistema, podr\u00eda contener c\u00f3digo malicioso que podr\u00eda explotar vulnerabilidades en el sistema, como ataques XSS. Esto podr\u00eda permitir a los atacantes ejecutar scripts, robar datos o modificar el comportamiento del sistema. \n\nUn caso en el cual se podr\u00eda explotar esta vulnerabilidad se obtiene al filtrar :red[Action]: :green[click(Btn_Submit)], :red[paperID]: :green[NULL], :red[User ID]: :green[user135]. \n",
            "xss_1": "\nEste usuario descubri\u00f3 el m\u00ednimo de requerimientos que deben enviarse en cada secci\u00f3n del formulario para que este sea admitido por la aplicaci\u00f3n, como se observa hace env\u00edo de cadenas que no tiene un fin espec\u00edfico, esto puede dar pie futuros ataques xss que cumplan dichas caracter\u00edsticas que pueden afectar la integridad de la aplicaci\u00f3n y pueden tener consecuencias como extracci\u00f3n de datos y perdida absoluta del control de la misma. \n\nUna situacion similar ocurre con al filtrar :red[Action]: :green[click(Btn_Submit)], :red[paperID]: :green[NULL], :red[User ID]: :green[user048]\n"
        },
        "questions": "\n* Indicar qu\u00e9 informaci\u00f3n necesitar\u00edas para mejorar/ampliar tu entregable.\n\nBueno, creo que con un poco m\u00e1s de tiempo se podr\u00edan lograr grandes cambios en la aplicaci\u00f3n que planteo como soluci\u00f3n, ya que ser\u00eda una plataforma que minimizar\u00eda tiempo y esfuerzo a la hora de crear an\u00e1lisis exploratorio de datos con respecto a las formas tradicionales de hacerlo, incluso se podr\u00eda llegar un nivel m\u00e1s alto de automatizaci\u00f3n donde se generen informes que contextualicen al usuario de la informaci\u00f3n m\u00e1s importante del conjunto de datos.\n\nDe igual manera, se podr\u00eda crear una secci\u00f3n de modelos que pueda realizar pruebas y sacar m\u00e9tricas de estos de manera interactiva.\n\n* \u00bfC\u00f3mo crees que podr\u00eda mejorarse el dataset desde la creaci\u00f3n del mismo si lo tuvieses que procesar?\n\nSe podr\u00edan traer algunas variables clasificatorias para los usuarios que ayudar\u00edan a segmentar la poblaci\u00f3n de buena manera, por ejemplo:\n\n1. Edad\n2. Genero\n3. Profesion\n4. Informacion demografica\n\nTambien se puede establecer un protocolo de calidad de datos que puede ayudar a garantizar que los datos se recopilen y procesen de manera consistente y precisa, lo que aumenta la confiabilidad y la validez del dataset. Un claro ejemplo de esto ser\u00eda mejorar el formato en que llega la variable :blue[content] el cual podr\u00eda venir en formato :red[JSON] facilitando la manipulaci\u00f3n del mismo\n\n* \u00bfCu\u00e1les ser\u00edan las ventajas de poseer labels en este dataset?\n\nLa ventaja principal es que podr\u00edan implementar t\u00e9cnicas de aprendizaje supervisado, esto tambi\u00e9n ayuda a comprender mejor la estructura y patrones de los datos. De igual manera, pueden mejorar la precisi\u00f3n de los resultados obtenidos en los modelos, al igual que hay un ahorro de tiempo y recursos para evitar etiquetar datos manualmente o con t\u00e9cnicas de aprendizaje no supervisado.\n\n* \u00bfDe poseer a\u00fan m\u00e1s datos relacionados con el consumo de dicho site, crees que podr\u00edamos identificar ataques sobre el mismo?\u00bfDe qu\u00e9 tipo?.\n\nSi, en lo absoluto, a pesar de que ya se tiene una gran cantidad de datos, puede que en un caso de uso donde se busque entender el comportamiento de cada usuario estos datos se queden cortos, en un sistema de modelos escalonados donde primero se determina si un evento es an\u00f3malo entre todo el dataset o de todos los eventos generados y luego un modelo especifico por usuario el tener m\u00e1s datos por usuario mejora el entendimiento de la l\u00ednea base de comportamiento del mismo.\n"
    }
}